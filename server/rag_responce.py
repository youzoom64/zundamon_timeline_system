# QueryRefinerRAG.py
import os
os.environ['GRPC_VERBOSITY'] = 'ERROR'
os.environ['GLOG_minloglevel'] = '2'

import sqlite3
import numpy as np
from typing import List, Dict, Optional
import json
from datetime import datetime

TARGET_USER_ID = "21639740"

class AIClient:
    def __init__(self, model_type: str, api_key: str):
        self.model_type = model_type
        self.api_key = api_key
        
        if model_type.startswith("openai"):
            import openai
            self.client = openai.OpenAI(api_key=api_key, timeout=20.0)
        elif model_type.startswith("google"):
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.genai = genai
            self.client = genai.GenerativeModel('gemini-2.5-flash')
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«: {model_type}")
    
    def chat_completion(self, messages: List[Dict], max_tokens: int = 800, temperature: float = 0.4) -> str:
        try:
            if self.model_type.startswith("openai"):
                model = "gpt-4o" if self.model_type == "openai-gpt4o" else "gpt-4o-mini"
                resp = self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return (resp.choices[0].message.content or "").strip()
            
            elif self.model_type.startswith("google"):
                if len(messages) >= 2:
                    system_content = messages[0].get("content", "")
                    user_content = messages[1].get("content", "")
                    prompt = f"{system_content}\n\n{user_content}"
                else:
                    prompt = messages[0].get("content", "")
                
                response = self.client.generate_content(prompt)
                return response.text.strip()
                
        except Exception as e:
            return f"AIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"

class RAGSearchSystem:
    def __init__(self, main_db_path: str = None, vector_db_path: str = None, config_path: str = None):
        # Aã‚·ã‚¹ãƒ†ãƒ ï¼ˆncv_special_monitorï¼‰ã®çµ¶å¯¾ãƒ‘ã‚¹è¨­å®š
        self.a_system_base = "C:/project_root/app_workspaces/ncv_special_monitor"

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’Aã‚·ã‚¹ãƒ†ãƒ ã«è¨­å®š
        self.main_db_path = main_db_path or f"{self.a_system_base}/data/ncv_monitor.db"
        self.vector_db_path = vector_db_path or f"{self.a_system_base}/data/vectors.db"
        self.config_path = config_path or f"{self.a_system_base}/config/ncv_special_config.json"

        if not os.path.exists(self.vector_db_path):
            print(f"âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.vector_db_path}")

        if not os.path.exists(self.main_db_path):
            print(f"âš ï¸ ãƒ¡ã‚¤ãƒ³DBãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.main_db_path}")

        self.config = self._load_config()

        # å„å‡¦ç†ç”¨ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å€‹åˆ¥ã«åˆæœŸåŒ–
        self.query_client = self._init_query_client()
        self.answer_client = self._init_answer_client()
        self.embedding_client = self._init_embedding_client()

    def _load_config(self) -> Dict:
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
        return {}
    
    def _init_query_client(self) -> AIClient:
        """è³ªå•æ•´å½¢ç”¨AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
        api_settings = self.config.get('api_settings', {})
        model_type = api_settings.get('query_ai_model', 'openai-gpt4o-mini')
        
        if model_type.startswith("openai") or model_type in ["gpt-4o", "gpt-4o-mini"]:
            api_key = api_settings.get('openai_api_key') or os.getenv('OPENAI_API_KEY')
            if not model_type.startswith("openai"):
                model_type = f"openai-{model_type}"
        elif model_type.startswith("google") or "gemini" in model_type:
            api_key = api_settings.get('google_api_key') or os.getenv('GOOGLE_API_KEY')
            if not model_type.startswith("google"):
                model_type = f"google-{model_type}"
        else:
            raise RuntimeError(f"âŒ æœªå¯¾å¿œã®ã‚¯ã‚¨ãƒªãƒ¢ãƒ‡ãƒ«: {model_type}")
        
        if not api_key:
            raise RuntimeError("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ” è³ªå•æ•´å½¢ãƒ¢ãƒ‡ãƒ«: {model_type}")
        return AIClient(model_type, api_key)
    
    def _init_answer_client(self) -> AIClient:
        """å›ç­”ç”Ÿæˆç”¨AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
        api_settings = self.config.get('api_settings', {})
        model_type = api_settings.get('answer_ai_model', 'openai-gpt4o')
        
        if model_type.startswith("openai") or model_type in ["gpt-4o", "gpt-4o-mini"]:
            api_key = api_settings.get('openai_api_key') or os.getenv('OPENAI_API_KEY')
            if not model_type.startswith("openai"):
                model_type = f"openai-{model_type}"
        elif model_type.startswith("google") or "gemini" in model_type:
            api_key = api_settings.get('google_api_key') or os.getenv('GOOGLE_API_KEY')
            if not model_type.startswith("google"):
                model_type = f"google-{model_type}"
        else:
            raise RuntimeError(f"âŒ æœªå¯¾å¿œã®å›ç­”ãƒ¢ãƒ‡ãƒ«: {model_type}")
        
        if not api_key:
            raise RuntimeError("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ’¡ å›ç­”ç”Ÿæˆãƒ¢ãƒ‡ãƒ«: {model_type}")
        return AIClient(model_type, api_key)
    
    def _init_embedding_client(self):
        """åŸ‹ã‚è¾¼ã¿ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆè¨­å®šã‹ã‚‰é¸æŠï¼‰"""
        api_settings = self.config.get('api_settings', {})
        embedding_model = api_settings.get('embedding_model', 'text-embedding-3-small')
        
        if embedding_model in ['text-embedding-3-small', 'text-embedding-3-large', 'text-embedding-ada-002']:
            # OpenAI Embedding
            api_key = api_settings.get('openai_api_key') or os.getenv('OPENAI_API_KEY')
            if not api_key:
                raise RuntimeError("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            import openai
            self.embedding_client_type = 'openai'
            self.embedding_model = embedding_model
            self.openai_client = openai.OpenAI(api_key=api_key)
            print(f"ğŸ”— åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {embedding_model} (OpenAI)")
            
        elif embedding_model in ['models/text-embedding-004', 'models/embedding-001']:
            # Google Embedding
            api_key = api_settings.get('google_api_key') or os.getenv('GOOGLE_API_KEY')
            if not api_key:
                raise RuntimeError("âŒ Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.embedding_client_type = 'google'
            self.embedding_model = embedding_model
            self.genai = genai
            print(f"ğŸ”— åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {embedding_model} (Google)")
            
        else:
            raise RuntimeError(f"âŒ æœªå¯¾å¿œã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {embedding_model}")

    def preprocess_question(self, question: str) -> str:
        messages = [
            {
                "role": "system", 
                "content": """ã‚ãªãŸã¯æ¤œç´¢ã‚¯ã‚¨ãƒªå¤‰æ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
å…¥åŠ›ã•ã‚ŒãŸè³ªå•æ–‡ã‚’ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«é©ã—ãŸçŸ­ã„æ–‡ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
ãƒ«ãƒ¼ãƒ«:
1. ç–‘å•è©ã‚„åŠ©è©ã‚’å‰Šé™¤ã—ã¦ã‚ˆã„ãŒã€æ„å‘³ä¸Šã®ä¸»èªãƒ»å¯¾è±¡ãƒ»è¡Œç‚ºã¯å¿…ãšæ®‹ã™
2. å›ºæœ‰åè©ã¯çœç•¥ã›ãšä¿æŒã™ã‚‹
3. æ›–æ˜§ãªäººç‰©å‚ç…§ï¼ˆã“ã®äººã€ã“ã„ã¤ç­‰ï¼‰ã¯ user_id=21639740 ã«ç½®ãæ›ãˆã‚‹
4. å‡ºåŠ›ã¯ä¸€æ–‡ã®ã¿ã§ã€ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦
5. å‡ºåŠ›å½¢å¼ã¯è‡ªç„¶ãªçŸ­æ–‡ã§ã‚ˆã„ï¼ˆå˜èªã®ç¾…åˆ—ã¯ç¦æ­¢ï¼‰"""
            },
            {"role": "user", "content": question}
        ]
        
        refined = self.query_client.chat_completion(messages, max_tokens=50, temperature=0)
        if not refined or "ã‚¨ãƒ©ãƒ¼" in refined:
            refined = question
        
        print(f"ğŸ§­ è³ªå•æ•´å½¢: {question} â†’ {refined}")
        return refined

    def _get_embedding(self, text: str) -> np.ndarray:
        """è¨­å®šã«åŸºã¥ãåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        try:
            if self.embedding_client_type == 'openai':
                resp = self.openai_client.embeddings.create(
                    model=self.embedding_model,
                    input=text
                )
                return np.array(resp.data[0].embedding, dtype=np.float32)
            
            elif self.embedding_client_type == 'google':
                result = self.genai.embed_content(
                    model=self.embedding_model,
                    content=text
                )
                return np.array(result['embedding'], dtype=np.float32)
                
        except Exception as e:
            print(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒæ•°ã‚’å‹•çš„ã«æ±ºå®š
            if self.embedding_client_type == 'openai':
                return np.zeros(1536, dtype=np.float32)
            else:
                return np.zeros(768, dtype=np.float32)

    def _search_similar_comments(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        results: List[Dict] = []
        try:
            with sqlite3.connect(self.vector_db_path) as vconn:
                cur = vconn.cursor()
                cur.execute("""
                    SELECT cv.comment_id, cv.user_id, cv.comment_text, cv.vector_data, cv.broadcast_id
                    FROM comment_vectors cv
                    WHERE cv.user_id = ?
                """, (TARGET_USER_ID,))
                rows = cur.fetchall()

            for comment_id, uid, comment_text, vector_blob, broadcast_id in rows:
                stored = np.frombuffer(vector_blob, dtype=np.float32)
                sim = self._cosine_similarity(query_vector, stored)
                results.append({
                    "comment_id": comment_id,
                    "user_id": uid,
                    "comment_text": comment_text,
                    "broadcast_id": broadcast_id,
                    "similarity": sim,
                })

            results.sort(key=lambda x: x["similarity"], reverse=True)
            results = results[:top_k]
            results = self._enrich_comment_results(results)

            print(f"ğŸ’¬ é¡ä¼¼ã‚³ãƒ¡ãƒ³ãƒˆ: {len(results)}ä»¶ (user_id={TARGET_USER_ID})")
            return results

        except Exception as e:
            print(f"âŒ ã‚³ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _enrich_comment_results(self, items: List[Dict]) -> List[Dict]:
        if not items:
            return items
        try:
            ids = [i["comment_id"] for i in items]
            placeholders = ",".join("?" for _ in ids)
            with sqlite3.connect(self.main_db_path) as conn:
                cur = conn.cursor()
                cur.execute(f"""
                    SELECT c.id, c.user_name, c.timestamp, c.elapsed_time,
                           b.lv_value, b.live_title, b.start_time,
                           su.display_name
                    FROM comments c
                    JOIN broadcasts b ON c.broadcast_id = b.id
                    LEFT JOIN special_users su ON c.user_id = su.user_id
                    WHERE c.id IN ({placeholders})
                """, ids)
                meta = {row[0]: {
                    "user_name": row[1],
                    "timestamp": row[2],
                    "elapsed_time": row[3],
                    "lv_value": row[4],
                    "live_title": row[5],
                    "start_time": row[6],
                    "display_name": row[7]
                } for row in cur.fetchall()}

            for it in items:
                info = meta.get(it["comment_id"], {})
                it.update(info)
            return items
        except Exception as e:
            print(f"âš ï¸ ã‚³ãƒ¡ãƒ³ãƒˆä»˜éšæƒ…å ±ã®å–å¾—ã«å¤±æ•—: {e}")
            return items

    def _cosine_similarity(self, v1: np.ndarray, v2: np.ndarray) -> float:
        dot = float(np.dot(v1, v2))
        n1 = float(np.linalg.norm(v1))
        n2 = float(np.linalg.norm(v2))
        return dot / (n1 * n2) if n1 and n2 else 0.0

    def _build_context(self, comments: List[Dict]) -> str:
        parts: List[str] = []

        if comments:
            parts.append("ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã€‘")
            for i, c in enumerate(comments, 1):
                disp = (c.get("display_name") or c.get("user_name") or "").strip()
                if disp.startswith("ãƒ¦ãƒ¼ã‚¶ãƒ¼") and disp.replace("ãƒ¦ãƒ¼ã‚¶ãƒ¼", "").isdigit():
                    disp = (c.get("user_name") or disp).strip()

                start_time_str = "ä¸æ˜"
                if c.get("start_time"):
                    try:
                        start_time_str = datetime.fromtimestamp(int(c["start_time"])).strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        start_time_str = "ä¸æ˜"

                parts.append(
                    f"{i}. ãƒ¦ãƒ¼ã‚¶ãƒ¼: {disp or 'ä¸æ˜'} (ID: {c.get('user_id')})"
                    f"\n   ã‚³ãƒ¡ãƒ³ãƒˆ: ã€Œ{c.get('comment_text','')}ã€"
                    f"\n   é…ä¿¡: {c.get('live_title','ä¸æ˜ãªé…ä¿¡')} / é–‹å§‹: {start_time_str} / çµŒé: {c.get('elapsed_time','?')} [é¡ä¼¼åº¦: {c.get('similarity',0):.3f}]"
                )

        return "\n\n".join(parts)

    def _generate_answer(self, question: str, context: str) -> str:
        messages = [
            {
                "role": "system",
                "content": """ã‚ãªãŸã¯ãƒ‹ã‚³ãƒ‹ã‚³ç”Ÿæ”¾é€ã®åˆ†ææ¡ˆå†…æ‹…å½“ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãƒ»åˆ†æï¼‰ã«ã®ã¿åŸºã¥ã„ã¦ã€è©³ç´°ã‹ã¤æ ¹æ‹ ä»˜ãã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
å›ç­”ã«ã¯ä»¥ä¸‹ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ï¼š
1. user_id ã¨åå‰ã‚’æ˜è¨˜
2. å…·ä½“çš„ãªã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’å¼•ç”¨
3. æ”¾é€ã‚¿ã‚¤ãƒˆãƒ«å
4. é…ä¿¡é–‹å§‹æ—¥æ™‚
5. é…ä¿¡é–‹å§‹ã‹ã‚‰ã®çµŒéæ™‚é–“
6. ã©ã®ç™ºè¨€ãŒã„ã¤ã®é…ä¿¡ã§ã®ã‚‚ã®ã‹ã‚’æ˜ç¢ºã«ç¤ºã™

å„ã‚³ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦3ã¤ç¨‹åº¦ç”¨æ„ã—ã€ã€Œé…ä¿¡ã€ã‚¿ã‚¤ãƒˆãƒ«åã€(é–‹å§‹: æ—¥æ™‚)ã®çµŒéXXåˆ†ã§ã®ã‚³ãƒ¡ãƒ³ãƒˆã€å†…å®¹ã€ã€ã®å½¢å¼ã§è©³ç´°ã‚’50æ–‡å­—ç¨‹åº¦ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
ä¸è¶³ãŒã‚ã‚‹å ´åˆã¯ã€ä¸è¶³ã—ã¦ã„ã‚‹ã€ã¨è¿°ã¹ã¦ãã ã•ã„ã€‚
ãšã‚“ã ã‚‚ã‚“ã®å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚èªå°¾ã¯å¿…ãšãªã®ã ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚
å…¨ä½“ã§300æ–‡å­—ç¨‹åº¦ä»¥å†…ã«ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""
            },
            {
                "role": "user", 
                "content": f"è³ªå•: {question}\n\nå‚è€ƒæƒ…å ±:\n{context}\n\nã“ã®æƒ…å ±ã ã‘ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
            }
        ]
        
        return self.answer_client.chat_completion(messages)

    def search_and_answer(self, question: str, top_k: int = 10) -> str:
        print(f"ğŸ” è³ªå•: {question}")
        print(f"ğŸ§­ å›ºå®š user_id={TARGET_USER_ID} ã‚’ä½¿ç”¨")

        refined = self.preprocess_question(question)
        query_vec = self._get_embedding(refined)
        comments = self._search_similar_comments(query_vec, top_k)

        context = self._build_context(comments)
        if not context.strip():
            print("ğŸ§­ æ¤œç´¢çµæœ: 0ä»¶")
            return "ğŸ¤· é–¢é€£æƒ…å ±ãªã—"

        print(f"ğŸ“Š æ¤œç´¢çµæœ: ã‚³ãƒ¡ãƒ³ãƒˆ{len(comments)}ä»¶")
        return self._generate_answer(question, context)


if __name__ == "__main__":
    import sys
    rag = RAGSearchSystem()

    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])
    else:
        question = "ã“ã®äººãªã«ãŒå¥½ãï¼Ÿ"

    ans = rag.search_and_answer(question)
    print(f"\nğŸ’¡ å›ç­”:\n{ans}")